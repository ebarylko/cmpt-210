## Question one
Let $I_A$ and $I_B$ be the indicator variables for events A and B. Prove that $I_A$ and $I_B$
are independent iff A and B are independent.

Forward direction: prove $I_A, I_B$ independent $\rightarrow A, B$ independent

If $I_A$ and $I_B$ are independent, this means that $Pr(I_A = c \cap I_B = d) = Pr(I_A = c)Pr(I_B = d), c, d \in \set{0, 1}$

We know that $Pr(I_A = 1) = Pr(A)$, and $Pr(I_A = 0) = Pr(A^{\complement})$

Similarly, $Pr(I_B = 1) = Pr(B)$, and $Pr(I_B = 0) = Pr(B^{\complement})$

If c = d = 1, then the expression
$Pr(I_A = c \cap I_B = d) = Pr(I_A = c)Pr(I_B = d)$ becomes
$Pr(A \cap B) = Pr(A)Pr(B)$. Since $Pr(A \cap B) = Pr(A)Pr(B)$, this means that 
$A$ and $B$ are independent.

Backwards directions: prove $A, B$ independent $\rightarrow I_A, I_B$ independent

If $A$ and $B$ are independent, then that means $Pr(A \cap B) = Pr(A)Pr(B)$.

As a result of this, we also have that $Pr(A^{\complement}B^{\complement}) = Pr(A^{\complement})Pr(B^{\complement})$

We know that for $I_A$, $Pr(I_A = 1) = Pr(A)$ and $Pr(I_A = 0) = Pr(A^{\complement})$

Similarly, for  $I_B$, $Pr(I_B = 1) = Pr(B)$ and $Pr(I_B = 0) = Pr(B^{\complement})$

Therefore, we need to prove that for any combination of $c, d, Pr(I_A = c \cap I_B = d) = Pr(I_A = c)Pr(I_B = d)$

We already know that $Pr(AB) = Pr(A)Pr(B)$ from the premise of the question. We now need to 
prove that $Pr(A^{\complement}B) = Pr(A^{\complement})Pr(B), Pr(B^{\complement}A) = Pr(B^{\complement})Pr(A)$

I will now prove that $Pr(A^{\complement}B) = Pr(A^{\complement})Pr(B)$

$Pr(A^{\complement}B)  = Pr(A (1 - Pr(B))$

$Pr(A (1 - Pr(B)) = Pr(A) - Pr(AB))$

$Pr(A) - Pr(AB)) = Pr(A) - Pr(A)Pr(B)$ (by the fact of A and B being independent)

$Pr(A) - Pr(A)Pr(B) = Pr(A)(1 - Pr(B))$

By using the fact that $Pr(B^{\complement}) = 1 - Pr(B)$, we can simplify the above expression.

$Pr(A) - Pr(A)Pr(B) = Pr(A)Pr(B^{\complement})$

We have proved that $Pr(A^{\complement}B) = Pr(A^{\complement})Pr(B)$.

A similar argument can be made to prove that $Pr(B^{\complement}A) = Pr(B^{\complement})Pr(A)$.

$Pr(B^{\complement}A)  = Pr(B (1 - Pr(A))$

$Pr(B (1 - Pr(A)) = Pr(B) - Pr(AB))$

$Pr(B) - Pr(AB)) = Pr(B) - Pr(A)Pr(B)$ (by the fact of A and B being independent)

$Pr(B) - Pr(A)Pr(B) = Pr(B)(1 - Pr(A))$

By using the fact that $Pr(A^{\complement}) = 1 - Pr(A)$, we can simplify the above expression.

$Pr(B)(1 - Pr(A)) = Pr(B)Pr(A^{\complement})$

We have proved that $Pr(A^{\complement}B) = Pr(A^{\complement})Pr(B)$.

I will now prove that $Pr(B^{\complement}A^{\complement}) = Pr(B^{\complement})Pr(A^{\complement})$

Since $A$ and $B$ are independent, this implies that $Pr(AB) = Pr(A)Pr(B)$.

$Pr({A}^{\complement}{B}^{\complement}) = \overline{Pr(A \cup B)}$ (by De Morgan)

$Pr({A}^{\complement}{B}^{\complement}) = \overline{Pr(A \cup B)}$

Expanding out $\overline{Pr(A \cup B)}$, we obtain $\overline{Pr(A) + Pr(B) - Pr(A \cap B)}$,
which is equivalent to  $1 - Pr(A) - Pr(B) + Pr(A \cap B)$.

We now have that
$Pr({A}^{\complement}{B}^{\complement})  = 1 - Pr(A) - Pr(B) + Pr(A \cap B)$

We can rewrite the expression as
$Pr({A}^{\complement}{B}^{\complement})  = (1 - Pr(A))(1 - Pr(B))$, which is
equivalent to  
$Pr({A}^{\complement}{B}^{\complement})  = Pr({A}^{\complement})Pr({B}^{\complement})$.

The above statement proves that $A$ and $B$ being independent implies the
independence of ${A}^{\complement}$ and ${B}^{\complement}$.


Since I have shown that for any combination of $c, d, Pr(I_A = c \cap I_B = d) = Pr(I_A = c)Pr(I_B = d)$,
this implies that $A$ and $B$ being independent implies that $I_A$ and $I_B$ are independent.
## Question two

## Question four
Suppose R, S, and T be mutually independent random variables on the same probability 
space with uniform distribution on the range [1, 2, 3]
Let $M = max{R, S, T}$. Compute the values of the probability density function,
$PDF_M$ , of M 

Since $R, S$ and $T$ are uniformly distributed over [1, 2, 3], this means that 
$\forall Q \in \set{R, S, T}, $Pr(Q = 1) = Pr(Q = 2) = Pr(Q = 3) = \frac{1}{3}$

Since $R, S$ and $T$ are mutually independent, this means that $Pr(R = a \cap S = b \cap T = c)$
= $Pr(R = a)Pr(S = b)Pr(T = c), a, b, c \in \set{1, 2, 3}$.

This means that for any sequence within $\set{1, 2, 3}^3$, the probability of obtaining that 
sequence is ${\frac{1}{3}}^3 = \frac{1}{27}$.

For calculating $PDF_M$, I must find the values $Pr(M = 1), Pr(M = 2)$, and $Pr(M = 3)$.

For calculating $Pr(M = 1)$, I must find the probability of obtaining a sequence where the 
maximum value is 1. There is only one such possible sequence, being (1, 1, 1). 
As a consequence of this, $Pr(M = 1) = \frac{1}{27}$.

For calculating $Pr(M = 2)$, I must find the probability of obtaining a sequence where the
maximum value is 2. This can be done by selecting one, two, or three of the random variables 
to hold the value 2. 
We can do this in 3 + 3 + 1 = 7 ways. As a result of this,  $Pr(M = 2) = \frac{7}{27}$.

For calculating $Pr(M = 3)$, I can use the following relation: 1 = $Pr(M = 1) + Pr(M = 3) + Pr(M = 3)$.
I can rearrange the terms to obtain $Pr(M = 3)$ obtaining $1 - Pr(M = 1) - Pr(M = 2)$. Doing this, 
my value of $Pr(M = 3)$ is $\frac{19}{27}$.

## Question nine

### What is the probability that Bruce breaks exactly 2 out of the 5 boards that are placed before him?

Since the probability of breaking the boards is mutually independent and the probability of 
breaking a board remains constant, we can model the probability of breaking x boards using 
a binomial distribution.

Let $Pr(X)$ denote the event that Bruce breaks $X$ boards.

$Pr(X) = \binom{5}{X}{0.8}^X{0.2)^{5 - X}$

For $Pr(2)$, we obtain $\binom{5}{2}{0.8}^2{0.2}^3$

### What is the probability that Bruce breaks at most 3 out of the 5 boards that are placed before him?
I am asked to obtain $Pr(X \le 3)$. To do this, I can take the complement of $Pr(X \ge 4)$.

To calculate $Pr(X \ge 4)$, I sum up the values of $Pr(X = 4)$ and $Pr(X = 5)$